{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"association_rules_generator.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Step1: Mount the drive and install required packages"],"metadata":{"id":"VXkQNVH07W5N"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dnXQqsWj7GDG","outputId":"9d219675-2e75-4e1f-965f-9df22253bfe4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Collecting efficient_apriori\n","  Downloading efficient_apriori-2.0.1-py3-none-any.whl (14 kB)\n","Installing collected packages: efficient-apriori\n","Successfully installed efficient-apriori-2.0.1\n","Collecting pyvis\n","  Downloading pyvis-0.1.9-py3-none-any.whl (23 kB)\n","Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from pyvis) (5.5.0)\n","Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.7/dist-packages (from pyvis) (2.6.3)\n","Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.7/dist-packages (from pyvis) (2.11.3)\n","Collecting jsonpickle>=1.4.1\n","  Downloading jsonpickle-2.0.0-py2.py3-none-any.whl (37 kB)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (4.8.0)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (1.0.18)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (2.6.1)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (57.4.0)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (0.8.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.9.6->pyvis) (2.0.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonpickle>=1.4.1->pyvis) (4.8.2)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.3.0->pyvis) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.3.0->pyvis) (0.2.5)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonpickle>=1.4.1->pyvis) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonpickle>=1.4.1->pyvis) (3.6.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython>=5.3.0->pyvis) (0.7.0)\n","Installing collected packages: jsonpickle, pyvis\n","Successfully installed jsonpickle-2.0.0 pyvis-0.1.9\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install efficient_apriori\n","!pip install pyvis"]},{"cell_type":"markdown","source":["Step 2: Import packages"],"metadata":{"id":"QFvGGzR770Gf"}},{"cell_type":"code","source":["import string\n","import pandas as pd\n","import numpy as np\n","import copy\n","import re\n","from efficient_apriori import apriori\n","from keras.preprocessing.text import text_to_word_sequence\n","from nltk import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')\n","from pyvis.network import Network\n","import networkx as nx\n","!ln -s /usr/local/share/jupyter/nbextensions /nbextensions\n","%cd /nbextensions\n","!wget -q https://upload.wikimedia.org/wikipedia/commons/3/37/Youtube.svg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cm6PWJ6k7MeN","outputId":"7b6fe515-a6c7-451c-ae7f-26a57d87e540"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","/usr/local/share/jupyter/nbextensions\n"]}]},{"cell_type":"markdown","source":["Step 3: Create useful functions"],"metadata":{"id":"ipaOrpc273mt"}},{"cell_type":"code","source":["stop = stopwords.words('english')\n","\n","class BaseTokenizer(object):\n","    def process_text(self, text):\n","        raise NotImplemented\n","\n","    def process(self, texts):\n","        for text in texts:\n","            yield self.process_text(text)\n","\n","\n","RE_PATTERNS = {\n","    ' american ':\n","        [\n","            'amerikan'\n","        ],\n","\n","    ' adolf ':\n","        [\n","            'adolf'\n","        ],\n","\n","\n","    ' hitler ':\n","        [\n","            'hitler'\n","        ],\n","\n","    ' fuck':\n","        [\n","            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n","            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n","            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n","            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n","            'feck ', ' fux ', 'f\\*\\*', \n","            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n","        ],\n","\n","    ' ass ':\n","        [\n","            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$'\n","                                                           '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n","            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s'\n","        ],\n","\n","    ' ass hole ':\n","        [\n","            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole'\n","        ],\n","\n","    ' bitch ':\n","        [\n","            'b[w]*i[t]*ch', 'b!tch',\n","            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n","            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h'\n","        ],\n","\n","    ' bastard ':\n","        [\n","            'ba[s|z]+t[e|a]+rd'\n","        ],\n","\n","    ' trans gender':\n","        [\n","            'transgender'\n","        ],\n","\n","    ' gay ':\n","        [\n","            'gay'\n","        ],\n","\n","    ' cock ':\n","        [\n","            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n","            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n","        ],\n","\n","    ' dick ':\n","        [\n","            ' dick[^aeiou]', 'deek', 'd i c k'\n","        ],\n","\n","    ' suck ':\n","        [\n","            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n","        ],\n","\n","    ' cunt ':\n","        [\n","            'cunt', 'c u n t'\n","        ],\n","\n","    ' bull shit ':\n","        [\n","            'bullsh\\*t', 'bull\\$hit'\n","        ],\n","\n","    ' homo sex ual':\n","        [\n","            'homosexual'\n","        ],\n","\n","    ' jerk ':\n","        [\n","            'jerk'\n","        ],\n","\n","    ' idiot ':\n","        [\n","            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots'\n","                                                                                      'i d i o t'\n","        ],\n","\n","    ' dumb ':\n","        [\n","            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n","        ],\n","\n","    ' shit ':\n","        [\n","            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t'\n","        ],\n","\n","    ' shit hole ':\n","        [\n","            'shythole'\n","        ],\n","\n","    ' retard ':\n","        [\n","            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n","        ],\n","\n","    ' rape ':\n","        [\n","            ' raped'\n","        ],\n","\n","    ' dumb ass':\n","        [\n","            'dumbass', 'dubass'\n","        ],\n","\n","    ' ass head':\n","        [\n","            'butthead'\n","        ],\n","\n","    ' sex ':\n","        [\n","            'sexy', 's3x', 'sexuality'\n","        ],\n","\n","\n","    ' nigger ':\n","        [\n","            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n","        ],\n","\n","    ' shut the fuck up':\n","        [\n","            'stfu'\n","        ],\n","\n","    ' pussy ':\n","        [\n","            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses'\n","        ],\n","\n","    ' faggot ':\n","        [\n","            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n","            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n","        ],\n","\n","    ' mother fucker':\n","        [\n","            ' motha ', ' motha f', ' mother f', 'motherucker',\n","        ],\n","\n","    ' whore ':\n","        [\n","            'wh\\*\\*\\*', 'w h o r e'\n","        ],\n","}\n","\n","\n","class PatternTokenizer(BaseTokenizer):\n","    def __init__(self, lower=True, initial_filters=r\"[^a-z0-9!@#\\$%\\^\\&\\*_\\-,\\.' ]\", patterns=RE_PATTERNS,\n","                 remove_repetitions=True):\n","        self.lower = lower\n","        self.patterns = patterns\n","        self.initial_filters = initial_filters\n","        self.remove_repetitions = remove_repetitions\n","\n","    def process_text(self, text):\n","        x = self._preprocess(text)\n","        for target, patterns in self.patterns.items():\n","            for pat in patterns:\n","                x = re.sub(pat, target, x)\n","        x = re.sub(r\"[^a-z' ]\", ' ', x)\n","        return x.split()\n","\n","    def process_ds(self, ds):\n","        ### ds = Data series\n","\n","        # lower\n","        ds = copy.deepcopy(ds)\n","        if self.lower:\n","            ds = ds.str.lower()\n","        # remove special chars\n","        if self.initial_filters is not None:\n","            ds = ds.str.replace(self.initial_filters, ' ')\n","        # fuuuuck => fuck\n","        if self.remove_repetitions:\n","            pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL) \n","            ds = ds.str.replace(pattern, r\"\\1\")\n","\n","        for target, patterns in self.patterns.items():\n","            for pat in patterns:\n","                ds = ds.str.replace(pat, target)\n","\n","        ds = ds.str.replace(r\"[^a-z' ]\", ' ')\n","\n","        return ds.str.split()\n","\n","    def _preprocess(self, text):\n","        # lower\n","        if self.lower:\n","            text = text.lower()\n","\n","        # remove special chars\n","        if self.initial_filters is not None:\n","            text = re.sub(self.initial_filters, ' ', text)\n","\n","        # fuuuuck => fuck\n","        if self.remove_repetitions:\n","            pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n","            text = pattern.sub(r\"\\1\", text)\n","        return text"],"metadata":{"id":"X4NCNKhu7zEI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 4: Read training data"],"metadata":{"id":"meklHksZ8Ggb"}},{"cell_type":"code","source":["file_name = \"/content/drive/MyDrive/Colab Notebooks/train.csv\"\n","data = pd.read_csv(file_name)    \n","tokenizer = PatternTokenizer()\n","data[\"comment_text\"] = tokenizer.process_ds(data[\"comment_text\"]).str.join(sep=\" \")"],"metadata":{"id":"O0DRqEm_79pQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 5: Clean data"],"metadata":{"id":"2Xz-J_bS8Ma8"}},{"cell_type":"code","source":["def clean_text(text):\n","    text = re.sub('[{}]'.format(string.punctuation), ' ', text.lower())\n","    return ' '.join([word for word in text.split() if word not in (stop)])\n","\n","data[\"comment_text\"] = data[\"comment_text\"].apply(clean_text)"],"metadata":{"id":"DrxY4lRW8F9A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 6: Generate association rules"],"metadata":{"id":"UCMK_1xP8Spn"}},{"cell_type":"code","source":["tags = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n","from collections import defaultdict\n","records = []\n","for tag in tags[:1]:\n","    print(f\"processing {tag}\")\n","    train = data[\"comment_text\"][data[tag] == 1]\n","    records.extend([text.split() for text in train])\n","itemsets, rules = apriori(records, min_support=0.005, min_confidence=0.7, verbosity=1)\n","result = defaultdict(set)\n","for rule in rules:\n","    left, right = rule.lhs, rule.rhs\n","    for left_side in left:\n","        for right_side in right:\n","            result[left_side].add(right_side)\n","    \n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zUU4A9BX8RtB","outputId":"ce71dc96-9a6e-4db4-9f53-8d9f63a310e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["processing toxic\n","Generating itemsets.\n"," Counting itemsets of length 1.\n","  Found 30828 candidate itemsets of length 1.\n","  Found 608 large itemsets of length 1.\n"," Counting itemsets of length 2.\n","  Found 184528 candidate itemsets of length 2.\n","  Found 1363 large itemsets of length 2.\n"," Counting itemsets of length 3.\n","  Found 7461 candidate itemsets of length 3.\n","  Found 262 large itemsets of length 3.\n"," Counting itemsets of length 4.\n","  Found 140 candidate itemsets of length 4.\n","  Found 4 large itemsets of length 4.\n"," Counting itemsets of length 5.\n","  Found 0 candidate itemsets of length 5.\n","Itemset generation terminated.\n","\n","Generating rules from itemsets.\n"," Generating rules of size 2.\n"," Generating rules of size 3.\n"," Generating rules of size 4.\n","Rule generation terminated.\n","\n","defaultdict(<class 'set'>, {'ahead': {'go'}, 'alone': {'leave'}, 'hole': {'ass', 'fuck'}, 'attacks': {'personal'}, 'bull': {'shit'}, 'unsigned': {'comment'}, 'ed': {'retard'}, 'mother': {'fuck'}, 'ual': {'homo', 'sex'}, 'org': {'wikipedia', 'http'}, 'ing': {'suck'}, 'piece': {'shit'}, 'ass': {'fuck'}, 'bitch': {'fuck'}, 'fuck': {'ass', 'suck', 'shit'}, 'get': {'ass'}, 'go': {'ass', 'fuck'}, 'like': {'ass'}, 'shit': {'ass', 'fuck'}, 'suck': {'ass'}, 'wikipedia': {'ass', 'shit'}, 'cock': {'suck'}, 'cunt': {'fuck'}, 'dick': {'fuck'}, 'edit': {'page'}, 'talk': {'page'}, 'sex': {'homo'}, 'homo': {'sex'}})\n"]}]},{"cell_type":"markdown","source":["Step 7: Gnerate output for creating dynamic html page"],"metadata":{"id":"JhZNvvuP8a49"}},{"cell_type":"code","source":["edges = set()\n","colors = [\n","\"Yellow\",\n","\"Blue\",\n","\"Red\",\n","\"Green\",\n","\"Black\",\n","\"Orange\",\n","\"Brown\",\n","\"Ivory\",\n","\"Teal\",\n","\"Silver\",\n","\"Purple\",\n","\"Navy blue\",\n","\"Pea green\",\n","\"Gray\",\n","\"Maroon\",\n","\"Charcoal\",\n","\"Aquamarine\",\n","\"Coral\",\n","\"Fuchsia\",\n","\"Wheat\",\n","\"Lime\",\n","\"Crimson\",\n","\"Khaki\",\n","\"Hot pink\",\n","\"Magenta\",\n","\"Olden\",\n","\"Plum\",\n","\"Olive\",\n","\"Cyan\"]\n","nodes = defaultdict(int)\n","for key, value in result.items():\n","    nodes[key] += len(value)\n","    for second in value:\n","        nodes[second] += 1\n","    for second in value:\n","        edges.add(tuple([key, second]))\n","count = 0\n","labels = dict()\n","for key, val in nodes.items():\n","    labels[key] = count\n","    color = colors[count % len(colors)]\n","    print(f'pyvis_graph.add_node({count}, label=\"{key}\", color=\"{color}\", size={val * 5})')\n","    count += 1\n","\n","count = 10\n","for u, v in edges:\n","    u = labels[u]\n","    v = labels[v]\n","    color = colors[count % len(colors)]\n","    count += 1\n","    print(f'pyvis_graph.add_edge({u}, {v}, size=10, color=\"{color}\")')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sdhi7YiF8YUI","outputId":"8f88fb4a-a625-49f5-a08b-6fe41c20077e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["pyvis_graph.add_node(0, label=\"ahead\", color=\"Yellow\", size=5)\n","pyvis_graph.add_node(1, label=\"go\", color=\"Blue\", size=15)\n","pyvis_graph.add_node(2, label=\"alone\", color=\"Red\", size=5)\n","pyvis_graph.add_node(3, label=\"leave\", color=\"Green\", size=5)\n","pyvis_graph.add_node(4, label=\"hole\", color=\"Black\", size=10)\n","pyvis_graph.add_node(5, label=\"ass\", color=\"Orange\", size=45)\n","pyvis_graph.add_node(6, label=\"fuck\", color=\"Brown\", size=55)\n","pyvis_graph.add_node(7, label=\"attacks\", color=\"Ivory\", size=5)\n","pyvis_graph.add_node(8, label=\"personal\", color=\"Teal\", size=5)\n","pyvis_graph.add_node(9, label=\"bull\", color=\"Silver\", size=5)\n","pyvis_graph.add_node(10, label=\"shit\", color=\"Purple\", size=30)\n","pyvis_graph.add_node(11, label=\"unsigned\", color=\"Navy blue\", size=5)\n","pyvis_graph.add_node(12, label=\"comment\", color=\"Pea green\", size=5)\n","pyvis_graph.add_node(13, label=\"ed\", color=\"Gray\", size=5)\n","pyvis_graph.add_node(14, label=\"retard\", color=\"Maroon\", size=5)\n","pyvis_graph.add_node(15, label=\"mother\", color=\"Charcoal\", size=5)\n","pyvis_graph.add_node(16, label=\"ual\", color=\"Aquamarine\", size=10)\n","pyvis_graph.add_node(17, label=\"homo\", color=\"Coral\", size=15)\n","pyvis_graph.add_node(18, label=\"sex\", color=\"Fuchsia\", size=15)\n","pyvis_graph.add_node(19, label=\"org\", color=\"Wheat\", size=10)\n","pyvis_graph.add_node(20, label=\"wikipedia\", color=\"Lime\", size=15)\n","pyvis_graph.add_node(21, label=\"http\", color=\"Crimson\", size=5)\n","pyvis_graph.add_node(22, label=\"ing\", color=\"Khaki\", size=5)\n","pyvis_graph.add_node(23, label=\"suck\", color=\"Hot pink\", size=20)\n","pyvis_graph.add_node(24, label=\"piece\", color=\"Magenta\", size=5)\n","pyvis_graph.add_node(25, label=\"bitch\", color=\"Olden\", size=5)\n","pyvis_graph.add_node(26, label=\"get\", color=\"Plum\", size=5)\n","pyvis_graph.add_node(27, label=\"like\", color=\"Olive\", size=5)\n","pyvis_graph.add_node(28, label=\"cock\", color=\"Cyan\", size=5)\n","pyvis_graph.add_node(29, label=\"cunt\", color=\"Yellow\", size=5)\n","pyvis_graph.add_node(30, label=\"dick\", color=\"Blue\", size=5)\n","pyvis_graph.add_node(31, label=\"edit\", color=\"Red\", size=5)\n","pyvis_graph.add_node(32, label=\"page\", color=\"Green\", size=10)\n","pyvis_graph.add_node(33, label=\"talk\", color=\"Black\", size=5)\n","pyvis_graph.add_edge(24, 10, size=10, color=\"Purple\")\n","pyvis_graph.add_edge(26, 5, size=10, color=\"Navy blue\")\n","pyvis_graph.add_edge(10, 5, size=10, color=\"Pea green\")\n","pyvis_graph.add_edge(29, 6, size=10, color=\"Gray\")\n","pyvis_graph.add_edge(9, 10, size=10, color=\"Maroon\")\n","pyvis_graph.add_edge(1, 6, size=10, color=\"Charcoal\")\n","pyvis_graph.add_edge(10, 6, size=10, color=\"Aquamarine\")\n","pyvis_graph.add_edge(0, 1, size=10, color=\"Coral\")\n","pyvis_graph.add_edge(15, 6, size=10, color=\"Fuchsia\")\n","pyvis_graph.add_edge(2, 3, size=10, color=\"Wheat\")\n","pyvis_graph.add_edge(4, 6, size=10, color=\"Lime\")\n","pyvis_graph.add_edge(4, 5, size=10, color=\"Crimson\")\n","pyvis_graph.add_edge(13, 14, size=10, color=\"Khaki\")\n","pyvis_graph.add_edge(25, 6, size=10, color=\"Hot pink\")\n","pyvis_graph.add_edge(5, 6, size=10, color=\"Magenta\")\n","pyvis_graph.add_edge(27, 5, size=10, color=\"Olden\")\n","pyvis_graph.add_edge(18, 17, size=10, color=\"Plum\")\n","pyvis_graph.add_edge(6, 10, size=10, color=\"Olive\")\n","pyvis_graph.add_edge(28, 23, size=10, color=\"Cyan\")\n","pyvis_graph.add_edge(1, 5, size=10, color=\"Yellow\")\n","pyvis_graph.add_edge(17, 18, size=10, color=\"Blue\")\n","pyvis_graph.add_edge(16, 18, size=10, color=\"Red\")\n","pyvis_graph.add_edge(19, 20, size=10, color=\"Green\")\n","pyvis_graph.add_edge(6, 23, size=10, color=\"Black\")\n","pyvis_graph.add_edge(30, 6, size=10, color=\"Orange\")\n","pyvis_graph.add_edge(16, 17, size=10, color=\"Brown\")\n","pyvis_graph.add_edge(7, 8, size=10, color=\"Ivory\")\n","pyvis_graph.add_edge(31, 32, size=10, color=\"Teal\")\n","pyvis_graph.add_edge(33, 32, size=10, color=\"Silver\")\n","pyvis_graph.add_edge(22, 23, size=10, color=\"Purple\")\n","pyvis_graph.add_edge(23, 5, size=10, color=\"Navy blue\")\n","pyvis_graph.add_edge(20, 5, size=10, color=\"Pea green\")\n","pyvis_graph.add_edge(19, 21, size=10, color=\"Gray\")\n","pyvis_graph.add_edge(20, 10, size=10, color=\"Maroon\")\n","pyvis_graph.add_edge(6, 5, size=10, color=\"Charcoal\")\n","pyvis_graph.add_edge(11, 12, size=10, color=\"Aquamarine\")\n"]}]},{"cell_type":"markdown","source":["Step 8: Run `association_rule_generator.py` to get dynamic html page"],"metadata":{"id":"kp_YgP1i89ac"}}]}